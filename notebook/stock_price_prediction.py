# -*- coding: utf-8 -*-
"""stock_price_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lwfbyTePKmDWtfH6JUE97D8SJIlgkch2

#Crowdsourcing Short Squeeze Dashboard
#Author: Musfira Mubeen
#Project Overview

This project predicts whether a stock will go up (1) or down (0) the next day.
It includes steps like data exploration, feature creation, model training, tuning, and saving the final model.

#Import Libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.preprocessing import  StandardScaler,MinMaxScaler
from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import  LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

"""#Load Data"""

from google.colab import files
files.upload()
df=pd.read_csv('/content/stock_features.csv')
print(df.head())

df.drop('date',axis=1,inplace=True)

"""#Machine Learning"""

#Split features and target
x=df.drop(['target_updown'],axis=1)
y=df['target_updown']

# Split dataset into training and testing
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

models={
    'LogisticRegression': LogisticRegression(),
    'DecisionTreeClassifier':DecisionTreeClassifier(),
    'RandomForestClassifier':RandomForestClassifier(),
    'GradientBoostingClassifier':GradientBoostingClassifier(),
    'SVM': SVC(),
    'KNeighborsClassifier': KNeighborsClassifier(),
    "Naive Bayas": GaussianNB()

}
Result={}
for name , model in models.items():
  model.fit(x_train,y_train)
  result=model.score(x_test,y_test)
  Result[name]=result
  print(f"{name} : {result}")

#pipeline
#preprocessing
standard=['open_price', 'high_price', 'low_price', 'close_price', 'adjclose_price', 'avg_price', 'MA10', 'MA50', 'VWAP', 'daily_return_pct', 'price_range']
minmax=['volume']
preprocessing=ColumnTransformer(transformers=[
    ('standard',StandardScaler(),standard),
    ('minmax',MinMaxScaler(),minmax)
], remainder='passthrough'
)
pipeline=Pipeline(steps=[
    ('preprocessing',preprocessing),
    ('Model',GradientBoostingClassifier(random_state=42))
])
pipeline.fit(x_train,y_train)
y_pred=pipeline.predict(x_test)
print(f"Accuracy Score: {accuracy_score(y_test,y_pred)}")
print(f"Confusion Matrix: {confusion_matrix(y_test,y_pred)}")
print(f"Classification Report: {classification_report(y_test,y_pred)}")

#Hyperparameter Tuning
param_grid={
    'Model__n_estimators': [50, 100, 150, 200],
    'Model__learning_rate': [0.01, 0.05, 0.1],
    'Model__max_depth': [3, 5, 7]

}
grid=GridSearchCV(pipeline,param_grid,cv=5,scoring='accuracy')
grid.fit(x_train,y_train)
print(f"Best Parameters: {grid.best_params_}")
print(f"Best Score: {grid.best_score_}")

best_model=grid.best_estimator_
y_pred=best_model.predict(x_test)
print(f"Accuracy Score: {accuracy_score(y_test,y_pred)}")
print(f"Confusion Matrix: {confusion_matrix(y_test,y_pred)}")
print(f"Classification Report: {classification_report(y_test,y_pred)}")
cm = confusion_matrix(y_test, y_pred)
# Plot Heatmap
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

#Cross Validation
scores=cross_val_score(best_model,x_train,y_train,cv=5,scoring='accuracy')
print(f"Cross Validation Scores: {scores}")
print(f"Mean CV Accuracy: {scores.mean()}")
print(f"Standard Deviation: {scores.std()}")

#Feature Importance
best_model = grid.best_estimator_
gb_model = best_model.named_steps['Model']
feature_names = standard + minmax
import matplotlib.pyplot as plt
import numpy as np

# Get importance values
importances = gb_model.feature_importances_

# Sort in descending order for clarity
indices = np.argsort(importances)[::-1]

# Plot
plt.figure(figsize=(10,6))
plt.bar(range(len(importances)), importances[indices], align='center')
plt.xticks(range(len(importances)), np.array(feature_names)[indices], rotation=45)
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importance in Predicting Stock Movement')
plt.tight_layout()
plt.show()

#save train model
joblib.dump(best_model,'final_stock_model.pkl')
print("Model Saved as final_stock_model.pkl")

#Load saved model for prediction
model=joblib.load('final_stock_model.pkl')
pred=model.predict(x_test[:5])
print(f"Prediction: {pred}")

"""#Final Notes
This project shows a complete machine learning workflow for stock movement prediction.
The tuned Gradient Boosting model gave strong and stable results.
Itâ€™s ready for deployment or further improvement with real-time data.
"""